{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.objectives import categorical_crossentropy\n",
    "from tensorflow.keras.activations import softmax\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from numpy.testing import assert_allclose\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_idx = chunk[-1]\n",
    "    target = tf.one_hot(target_idx, depth=34)\n",
    "    target = tf.reshape(target, [-1])\n",
    "    return input_text, target\n",
    "\n",
    "def preprocess_data():\n",
    "    path_to_file = \"./100k_SMILES.txt\"\n",
    "    text = open(path_to_file).read()\n",
    "\n",
    "    vocab = sorted(set(text))\n",
    "\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "\n",
    "    text_as_int = np.array([[char2idx[c]] for c in text])\n",
    "    \n",
    "    # The maximum length sentence you want for a single input in characters\n",
    "    seq_length = 137\n",
    "    examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "    # Create training examples / targets\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "        \n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # Batch size\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    # Buffer size to shuffle the dataset\n",
    "    # (TF data is designed to work with possibly infinite sequences,\n",
    "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "    # it maintains a buffer in which it shuffles elements).\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    return(dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "def get_compiled_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            CuDNNLSTM(128, input_shape=(137, 1), return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            CuDNNLSTM(256, return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            CuDNNLSTM(512, return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            CuDNNLSTM(256, return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            CuDNNLSTM(128),\n",
    "            Dropout(0.1),\n",
    "            Dense(34, activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./weights_2/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset = preprocess_data()\n",
    "history = model.fit(train_dataset, epochs=500, callbacks=callbacks_list)"
   ]
  }
 ]
}